{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73290,"databundleVersionId":8710574,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/suehuynh/academic-success-classification?scriptVersionId=186144660\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Preparation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd              # For data manipulation and analysis\nimport numpy as np               # For numerical computing\nfrom datetime import datetime\nfrom scipy.stats import trim_mean, kurtosis, skew, boxcox, yeojohnson      # For statistical analysis\nfrom math import sqrt\nimport matplotlib                # For plotting and visualization\nimport matplotlib.pyplot as plt  \nfrom pandas.plotting import parallel_coordinates\nimport seaborn as sns            # For statistical data visualization\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T18:46:13.714908Z","iopub.execute_input":"2024-06-29T18:46:13.715409Z","iopub.status.idle":"2024-06-29T18:46:13.827506Z","shell.execute_reply.started":"2024-06-29T18:46:13.715369Z","shell.execute_reply":"2024-06-29T18:46:13.826261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For machine learning\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, KFold,cross_val_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score,\n                             f1_score, confusion_matrix, classification_report)\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nimport optuna","metadata":{"execution":{"iopub.status.busy":"2024-06-29T18:46:15.648027Z","iopub.execute_input":"2024-06-29T18:46:15.648483Z","iopub.status.idle":"2024-06-29T18:46:15.65603Z","shell.execute_reply.started":"2024-06-29T18:46:15.648447Z","shell.execute_reply":"2024-06-29T18:46:15.654854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import dataset","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/playground-series-s4e6/train.csv', index_col = 0)\ndf_test = pd.read_csv('/kaggle/input/playground-series-s4e6/test.csv', index_col = 0)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T18:46:17.525352Z","iopub.execute_input":"2024-06-29T18:46:17.525786Z","iopub.status.idle":"2024-06-29T18:46:18.181656Z","shell.execute_reply.started":"2024-06-29T18:46:17.525757Z","shell.execute_reply":"2024-06-29T18:46:18.180462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:00:03.442897Z","iopub.execute_input":"2024-06-29T15:00:03.443578Z","iopub.status.idle":"2024-06-29T15:00:03.471757Z","shell.execute_reply.started":"2024-06-29T15:00:03.443538Z","shell.execute_reply":"2024-06-29T15:00:03.470779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:00:03.473043Z","iopub.execute_input":"2024-06-29T15:00:03.473356Z","iopub.status.idle":"2024-06-29T15:00:03.487485Z","shell.execute_reply.started":"2024-06-29T15:00:03.473329Z","shell.execute_reply":"2024-06-29T15:00:03.48645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n* Train dataset has 76,518 rows x 38 columns, of which 36 predictors, 1 ID column and `Target` obviously the target of the model.\n* Test dataset has 51,012 rows x 37 columns of 36 predictors and 1 ID column.\n* Both datasets are very structured and 'clean' with no missing values.\n* All the predictors are numerical (float and integers). Columns with float data points are for grades and continuous. Other Integer columns numerical-coded (one-hot code) for categories.\n* The `Target` of this model is **categorical** data, which will need further engineering.","metadata":{}},{"cell_type":"code","source":"df_train.describe().T.style","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:00:03.488604Z","iopub.execute_input":"2024-06-29T15:00:03.488939Z","iopub.status.idle":"2024-06-29T15:00:03.682025Z","shell.execute_reply.started":"2024-06-29T15:00:03.488909Z","shell.execute_reply":"2024-06-29T15:00:03.681034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization\n## Univariate Analysis","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10,6))\nsns.light_palette(\"seagreen\", as_cmap=True)\n\nax = sns.countplot(data = df_train,\n             x = 'Target',\n             palette = \"Spectral\")\nax.set_title('Distribution of each type of Target')\nax.set_xlabel('')\nax.set_ylabel('')\nax.set_yticklabels('')\nax.tick_params(left = False, bottom = False)\n\nax.bar_label(ax.containers[0], \n             labels = ['$ {:.1f}K'.format(value / 1000) for value in df_train['Target'].value_counts()],\n             label_type = 'edge',\n             color = 'black',\n             fontsize = 10)\n\nsns.despine(left = True, top = True)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:00:03.683399Z","iopub.execute_input":"2024-06-29T15:00:03.684102Z","iopub.status.idle":"2024-06-29T15:00:04.013555Z","shell.execute_reply.started":"2024-06-29T15:00:03.684064Z","shell.execute_reply":"2024-06-29T15:00:04.012534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(6, 6, figsize=(20, 25))\n \nfor i, column in enumerate(df_train.columns):\n    if column == 'Target':\n        continue\n    plt.subplots_adjust(top = 0.85)\n    ax = sns.histplot(data = df_train, \n                x = column, \n                bins = df_train[column].nunique(),\n                ax = axes[i // 6, i % 6])\n    \n    ax.set_yticklabels(['{:,.0f}K'.format(ticks / 1000) for ticks in ax.get_yticks()])\nfig.tight_layout(h_pad = 2)\nplt.subplots_adjust(top = 0.97)\nplt.suptitle('Distribution of Predictors', fontsize = 14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:00:04.015048Z","iopub.execute_input":"2024-06-29T15:00:04.015437Z","iopub.status.idle":"2024-06-29T15:00:16.485145Z","shell.execute_reply.started":"2024-06-29T15:00:04.0154Z","shell.execute_reply":"2024-06-29T15:00:16.483818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multivariate Analysis","metadata":{}},{"cell_type":"code","source":"# Correlation Matrix Heatmap\nfig, ax = plt.subplots(figsize = (25,15))\ndf_features = df_train.select_dtypes(include='number')\ncorr = df_features.corr()\nhm = sns.heatmap(corr,\n                annot = True,\n                ax = ax,\n                cmap = sns.color_palette(\"vlag\", as_cmap = True),\n                fmt = '.1f')\nfig.subplots_adjust(top = 0.95)\nplt.suptitle('Predictors Correlation Heatmap', fontsize = 14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:00:16.486499Z","iopub.execute_input":"2024-06-29T15:00:16.486845Z","iopub.status.idle":"2024-06-29T15:00:20.173827Z","shell.execute_reply.started":"2024-06-29T15:00:16.486809Z","shell.execute_reply":"2024-06-29T15:00:20.172714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n## Train/Test Split","metadata":{}},{"cell_type":"code","source":"X = df_train.drop(columns = 'Target' , axis = 1)\ny = df_train['Target']\n\nX_train , X_valid , y_train , y_valid = train_test_split(X , y , test_size = 0.2 , random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T18:46:29.511471Z","iopub.execute_input":"2024-06-29T18:46:29.512622Z","iopub.status.idle":"2024-06-29T18:46:29.565632Z","shell.execute_reply.started":"2024-06-29T18:46:29.512548Z","shell.execute_reply":"2024-06-29T18:46:29.564275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Label Encoder","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\n# Convert the target variable 'Target' to numerical data\ny_train = le.fit_transform(y_train)\ny_valid = le.transform(y_valid)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T18:46:31.710711Z","iopub.execute_input":"2024-06-29T18:46:31.711234Z","iopub.status.idle":"2024-06-29T18:46:31.741066Z","shell.execute_reply.started":"2024-06-29T18:46:31.711193Z","shell.execute_reply":"2024-06-29T18:46:31.739639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n### Idea 1 - Create new statistic features for the grade features","metadata":{}},{"cell_type":"code","source":"# SCORE_FEATURES = [col for col in df_test.columns if 'Curricular' in col]\n\n# def add_new_features(df : pd.DataFrame) -> pd.DataFrame:\n#     df['total'] = df[SCORE_FEATURES].sum(axis=1)\n#     df['mean'] = df[SCORE_FEATURES].mean(axis=1)\n#     df['std'] = df[SCORE_FEATURES].std(axis=1)\n#     df['max'] = df[SCORE_FEATURES].max(axis=1)\n#     df['min'] = df[SCORE_FEATURES].min(axis=1)\n#     df['median'] = df[SCORE_FEATURES].median(axis=1)\n#     df['q25'] = df[SCORE_FEATURES].quantile(0.25, axis=1)\n#     df['q75'] = df[SCORE_FEATURES].quantile(0.75, axis=1)\n    \n#     df['Curricular units 1yr (approved)'] = df['Curricular units 1st sem (approved)'] + df['Curricular units 2nd sem (approved)']\n#     df['Curricular units 1yr (grade)'] = df['Curricular units 1st sem (grade)'] + df['Curricular units 2nd sem (grade)']\n#     df['1st enrolled_credited'] = (df['Curricular units 1st sem (enrolled)'] - \n#                                          df['Curricular units 1st sem (credited)']) \n#     df['1st enrolled_approved'] = (df['Curricular units 1st sem (enrolled)'] - \n#                                          df['Curricular units 1st sem (approved)'])\n#     df['1st approved_credited'] = (df['Curricular units 1st sem (approved)'] - \n#                                          df['Curricular units 1st sem (credited)'])\n#     df['1st grade_total'] = ((df['Curricular units 1st sem (evaluations)'] - \n#                                     df['Curricular units 1st sem (without evaluations)']) * \n#                                    df['Curricular units 1st sem (grade)'])\n#     df['2nd enrolled_credited'] = (df['Curricular units 2nd sem (enrolled)'] - \n#                                          df['Curricular units 2nd sem (credited)']) \n#     df['2nd enrolled_approved'] = (df['Curricular units 2nd sem (enrolled)'] - \n#                                          df['Curricular units 2nd sem (approved)'])\n#     df['2nd approved_credited'] = (df['Curricular units 2nd sem (approved)'] - \n#                                          df['Curricular units 2nd sem (credited)'])\n#     df['2nd grade_total'] = ((df['Curricular units 2nd sem (evaluations)'] - \n#                                     df['Curricular units 2nd sem (without evaluations)']) * \n#                                    df['Curricular units 2nd sem (grade)'])\n#     return df\n\n# df_train = add_new_features(df_train)\n# df_test = add_new_features(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:00:20.243587Z","iopub.execute_input":"2024-06-29T15:00:20.243916Z","iopub.status.idle":"2024-06-29T15:00:20.249827Z","shell.execute_reply.started":"2024-06-29T15:00:20.243889Z","shell.execute_reply":"2024-06-29T15:00:20.248697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Idea 2 - Replace insignificant data","metadata":{}},{"cell_type":"code","source":"# df_train['Application mode'] =df_train['Application mode'].replace({12:np.NaN,\n#                                                            4:np.NaN,\n#                                                             35:np.NaN,\n#                                                             9:np.NaN,\n#                                                             3:np.NaN}\n#                                                            )\n# df_test['Application mode'] = df_test['Application mode'].replace({14:np.NaN,\n#                                                            35:np.NaN,\n#                                                             19:np.NaN,\n#                                                             3:np.NaN}\n#                                                            )\n# df_train['Course'] = df_train['Course'].replace({979:np.NaN,\n#                                          39:np.NaN })\n# df_test['Course'] = df_train['Course'].replace({7500:np.NaN,\n#                                          9257:np.NaN,\n#                                         2105:np.NaN,\n#                                          4147:np.NaN,})\n# df_train['Previous qualification'] = df_train['Previous qualification'].replace({37:np.NaN,\n#                                          36:np.NaN, \n#                                         17:np.NaN,\n#                                         11:np.NaN})\n# df_test['Previous qualification'] = df_test['Previous qualification'].replace({17:np.NaN,\n#                                                                        11:np.NaN,\n#                                                                        16:np.NaN})","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-29T15:00:20.251204Z","iopub.execute_input":"2024-06-29T15:00:20.25152Z","iopub.status.idle":"2024-06-29T15:00:20.26423Z","shell.execute_reply.started":"2024-06-29T15:00:20.251492Z","shell.execute_reply":"2024-06-29T15:00:20.263132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Results** - Ideas 1&2 did not improve the accuracy score so I move on with the orignal data","metadata":{}},{"cell_type":"markdown","source":"### Idea 3 - OneHotEncoder & StandardScaler","metadata":{}},{"cell_type":"code","source":"# Categorical columns: if the number of unique values is 8 or fewer\ncat_cols = [col for col in X.columns if X[col].nunique() <= 8]\n# Numerical columns: if the number of unique values is 9 or more\nnum_cols = [col for col in X.columns if X[col].nunique() >= 9]","metadata":{"execution":{"iopub.status.busy":"2024-06-29T18:46:42.80632Z","iopub.execute_input":"2024-06-29T18:46:42.806782Z","iopub.status.idle":"2024-06-29T18:46:42.872261Z","shell.execute_reply.started":"2024-06-29T18:46:42.806748Z","shell.execute_reply":"2024-06-29T18:46:42.870662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the preprocessing for numerical and categorical columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), num_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n    ])\n\n# Fit and transform the training data\nX_train_processed = preprocessor.fit_transform(X_train)\nX_valid_processed = preprocessor.transform(X_valid)\ndf_test_processed = preprocessor.transform(df_test)\n\n# Convert processed arrays back to DataFrames (optional)\nX_train_processed = pd.DataFrame(X_train_processed, columns=preprocessor.get_feature_names_out())\nX_valid_processed = pd.DataFrame(X_valid_processed, columns=preprocessor.get_feature_names_out())\ndf_test_processed = pd.DataFrame(df_test_processed, columns=preprocessor.get_feature_names_out())\n\n# Keep dataframe names consistent\nX_train = X_train_processed\nX_valid = X_valid_processed\ndf_test = df_test_processed","metadata":{"execution":{"iopub.status.busy":"2024-06-29T18:46:44.423228Z","iopub.execute_input":"2024-06-29T18:46:44.423656Z","iopub.status.idle":"2024-06-29T18:46:44.662008Z","shell.execute_reply.started":"2024-06-29T18:46:44.423623Z","shell.execute_reply":"2024-06-29T18:46:44.660826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling & Hyperparameter Tuning\nNext step, we will use Optuna to find the best hyperparameters for each algorithm and train/fit them.","metadata":{}},{"cell_type":"markdown","source":"## KNN","metadata":{}},{"cell_type":"code","source":"# # Define objective function for Optuna\n# def objective(trial):\n#     # Define hyperparameters to search\n#     params = {\n#         'n_neighbors': trial.suggest_int('n_neighbors', 1, 30),\n#         'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n#         'algorithm': trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n#         'leaf_size': trial.suggest_int('leaf_size', 10, 50),\n#         'p': trial.suggest_int('p', 1, 5)\n#     }\n\n#     # Split the training data into training and validation sets\n#     X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n#     # Train KNN model with current hyperparameters\n#     clf = KNeighborsClassifier(**params)\n#     clf.fit(X_train_split, y_train_split)\n\n#     # Predict on validation set\n#     y_pred = clf.predict(X_valid_split)\n\n#     # Calculate evaluation metric on validation set\n#     accuracy = accuracy_score(y_valid_split, y_pred)\n#     return accuracy\n\n# # Optimize hyperparameters using Optuna\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=50)\n\n# # Get best hyperparameters\n# best_params = study.best_params\n# print(\"Best Hyperparameters:\", best_params)\n\n# # Train final model with best hyperparameters\n# knn = KNeighborsClassifier(**best_params)\n# knn.fit(X_train, y_train)\n\n# # Predict on validation data\n# y_pred_knn = knn.predict(X_valid)\n\n# # Calculate accuracy on validation data\n# accuracy = accuracy_score(y_valid, y_pred_knn)\n# print(\"Accuracy on Validation Data:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:00:20.52843Z","iopub.execute_input":"2024-06-29T15:00:20.528839Z","iopub.status.idle":"2024-06-29T15:00:20.535844Z","shell.execute_reply.started":"2024-06-29T15:00:20.52878Z","shell.execute_reply":"2024-06-29T15:00:20.534695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Best KNN Parameters:**\n> {'n_neighbors': 19, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 26, 'p': 1}","metadata":{}},{"cell_type":"code","source":"# Create KNN model\nknn = KNeighborsClassifier(n_neighbors=19, \n                           weights='distance', \n                           algorithm='brute', \n                           leaf_size=26, \n                           p=1, \n                           metric='minkowski')\n\n# Perform cross-validation\ncv_scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n\n# Print CV scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean()) #0.7650211117908158\n\n# Fit the model on the full training data\nknn.fit(X_train, y_train)\n\n# Predict on validation data\ny_pred = knn.predict(X_valid)\n\n# Calculate accuracy on validation data\naccuracy = accuracy_score(y_valid, y_pred)\nprint(\"Accuracy on Validation Data:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T18:46:48.428861Z","iopub.execute_input":"2024-06-29T18:46:48.429656Z","iopub.status.idle":"2024-06-29T18:51:30.632783Z","shell.execute_reply.started":"2024-06-29T18:46:48.429617Z","shell.execute_reply":"2024-06-29T18:51:30.63128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"# import optuna\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score\n\n# # Define objective function for Optuna\n# def objective(trial):\n#     # Define hyperparameters to search\n#     params = {\n#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n#         'max_depth': trial.suggest_int('max_depth', 3, 20),\n#         'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n#         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n#         'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n#         'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n#         'random_state': 0\n#     }\n\n#     # Split the training data into training and validation sets\n#     X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n#     # Train Random Forest model with current hyperparameters\n#     clf = RandomForestClassifier(**params)\n#     clf.fit(X_train_split, y_train_split)\n\n#     # Predict on validation set\n#     y_pred = clf.predict(X_valid_split)\n\n#     # Calculate evaluation metric on validation set\n#     accuracy = accuracy_score(y_valid_split, y_pred)\n#     return accuracy\n\n# # Optimize hyperparameters using Optuna\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=50)\n\n# # Get best hyperparameters\n# best_params = study.best_params\n# print(\"Best Hyperparameters:\", best_params)\n\n# # Train final model with best hyperparameters\n# rf = RandomForestClassifier(**best_params)\n# rf.fit(X_train, y_train)\n\n# # Predict on validation data\n# y_pred_rf = rf.predict(X_valid)\n\n# # Calculate accuracy on validation data\n# accuracy = accuracy_score(y_valid, y_pred_rf)\n# print(\"Accuracy on Validation Data:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:05:01.289331Z","iopub.execute_input":"2024-06-29T15:05:01.289651Z","iopub.status.idle":"2024-06-29T15:05:01.297576Z","shell.execute_reply.started":"2024-06-29T15:05:01.289625Z","shell.execute_reply":"2024-06-29T15:05:01.296444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Random Forest model\nrf = RandomForestClassifier(\n    n_estimators=807,       \n    max_depth=19,            \n    max_features=0.33781074836721803,     \n    min_samples_split=14,    \n    min_samples_leaf=2,     \n    bootstrap=False,         \n    random_state=0         \n)\n\n# Perform cross-validation\ncv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')\n\n# Print CV scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean()) #0.8253340902649038\n\n# Fit the model on the full training data\nrf.fit(X_train, y_train)\n\n# Predict on validation data\ny_pred = rf.predict(X_valid)\n\n# Calculate accuracy on validation data\naccuracy = accuracy_score(y_valid, y_pred)\nprint(\"Accuracy on Validation Data:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T18:51:30.635498Z","iopub.execute_input":"2024-06-29T18:51:30.636003Z","iopub.status.idle":"2024-06-29T19:17:15.764489Z","shell.execute_reply.started":"2024-06-29T18:51:30.635958Z","shell.execute_reply":"2024-06-29T19:17:15.762797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"# import optuna\n# from xgboost import XGBClassifier\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score\n\n# # Define objective function for Optuna\n# def objective(trial):\n#     # Define hyperparameters to search\n#     params = {\n#         'booster': 'gbtree',\n#         'objective': 'multi:softmax',\n#         'eval_metric': 'merror',\n#         'max_depth': trial.suggest_int('max_depth', 3, 10),\n#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n#         'min_child_weight': trial.suggest_float('min_child_weight', 1, 10),\n#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n#         'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10.0),\n#         'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0),\n#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n#         'random_state': 0\n#     }\n\n#     # Split the training data into training and validation sets\n#     X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n#     # Train XGBoost model with current hyperparameters\n#     clf = XGBClassifier(**params)\n#     clf.fit(X_train_split, y_train_split)\n\n#     # Predict on validation set\n#     y_pred = clf.predict(X_valid_split)\n\n#     # Calculate evaluation metric on validation set\n#     accuracy = accuracy_score(y_valid_split, y_pred)\n#     return accuracy\n\n# # Optimize hyperparameters using Optuna\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=50)\n\n# # Get best hyperparameters\n# best_params = study.best_params\n# print(\"Best Hyperparameters:\", best_params)\n\n# # Train final model with best hyperparameters\n# xgb = XGBClassifier(**best_params)\n# xgb.fit(X_train, y_train)\n\n# # Predict on validation data\n# y_pred_xgb = xgb.predict(X_valid)\n\n# # Calculate accuracy on validation data\n# accuracy = accuracy_score(y_valid, y_pred_xgb)\n# print(\"Accuracy on Validation Data:\", accuracy)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-29T15:29:05.721865Z","iopub.execute_input":"2024-06-29T15:29:05.722253Z","iopub.status.idle":"2024-06-29T15:29:05.728672Z","shell.execute_reply.started":"2024-06-29T15:29:05.722217Z","shell.execute_reply":"2024-06-29T15:29:05.727656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Best XGB Hyperparameters:** \n> {'max_depth': 7, 'learning_rate': 0.04461514869110686, 'min_child_weight': 7.154603427477234, 'colsample_bytree': 0.6323856517233496, 'reg_alpha': 4.7491380136766965, 'reg_lambda': 7.78022631155423, 'subsample': 0.8577521128739977, 'n_estimators': 755}","metadata":{}},{"cell_type":"code","source":"# Create XGBoost model\nxgb_1 = XGBClassifier(booster = 'gbtree',\n                   objective='multi:softmax',\n                   eval_metric='merror',\n                   max_depth = 7,\n                   num_leaves = 250,\n                   reg_alpha = 0.518,\n                   reg_lambda = 0.113,\n                   learning_rate = 0.03,\n                   n_estimators = 900,\n                   min_child_weigh = 5.971,\n                   colsample_bytree = 0.662,\n                   subsample= 0.833, \n                   random_state = 0)\n\n# Perform cross-validation\ncv_scores = cross_val_score(xgb_1, X_train, y_train, cv=5, scoring='accuracy')\n\n# Print CV scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean()) #0.8311170920323452\n\n# Fit the model on the full training data\nxgb_1.fit(X_train, y_train)\n\n# Predict on validation data\ny_pred = xgb_1.predict(X_valid)\n\n# Calculate accuracy on validation data\naccuracy = accuracy_score(y_valid, y_pred)\nprint(\"Accuracy on Validation Data:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:17:15.766287Z","iopub.execute_input":"2024-06-29T19:17:15.766765Z","iopub.status.idle":"2024-06-29T19:26:28.244893Z","shell.execute_reply.started":"2024-06-29T19:17:15.766723Z","shell.execute_reply":"2024-06-29T19:26:28.243514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create XGBoost model\nxgb_2 = XGBClassifier(booster = 'gbtree',\n                   objective='multi:softmax',\n                   eval_metric='merror',\n                   grow_policy = 'depthwise',\n                   tree_method = 'hist',\n                   max_depth = 5,\n                   #num_leaves = 250,\n                   #reg_alpha = 0.518,\n                   reg_lambda = 29.548955808402486 ,\n                   learning_rate = 0.04104089631389812,\n                   n_estimators = 1311,\n                   min_child_weigh = 17.58377776073493,\n                   colsample_bytree = 0.4000772723424121,\n                   subsample= 0.9141573846486278, \n                   enable_categorical = True,\n                   gamma = 0)\n\n# Perform cross-validation\ncv_scores = cross_val_score(xgb_2, X_train, y_train, cv=5, scoring='accuracy')\n\n# Print CV scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean()) #0.8315254966736256\n\n# Fit the model on the full training data\nxgb_2.fit(X_train, y_train)\n\n# Predict on validation data\ny_pred = xgb_2.predict(X_valid)\n\n# Calculate accuracy on validation data\naccuracy = accuracy_score(y_valid, y_pred)\nprint(\"Accuracy on Validation Data:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:26:28.247708Z","iopub.execute_input":"2024-06-29T19:26:28.248083Z","iopub.status.idle":"2024-06-29T19:36:24.99303Z","shell.execute_reply.started":"2024-06-29T19:26:28.248051Z","shell.execute_reply":"2024-06-29T19:36:24.991611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create XGBoost model\nxgb_3 = XGBClassifier(booster = 'gbtree',\n                   objective='multi:softmax',\n                   eval_metric='merror',\n                   max_depth = 7,\n                   num_leaves = 250,\n                   reg_alpha = 4.7491380136766965,\n                   reg_lambda = 7.78022631155423,\n                   learning_rate = 0.044615148691106866,\n                   n_estimators = 755,\n                   min_child_weigh = 7.154603427477234,\n                   colsample_bytree = 0.6323856517233496,\n                   subsample= 0.8577521128739977, \n                   random_state = 0)\n# Perform cross-validation\ncv_scores = cross_val_score(xgb_3, X_train, y_train, cv=5, scoring='accuracy')\n\n# Print CV scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean()) #0.831198762018427\n\n# Fit the model on the full training data\nxgb_3.fit(X_train, y_train)\n\n# Predict on validation data\ny_pred = xgb_3.predict(X_valid)\n\n# Calculate accuracy on validation data\naccuracy = accuracy_score(y_valid, y_pred)\nprint(\"Accuracy on Validation Data:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:36:24.99435Z","iopub.execute_input":"2024-06-29T19:36:24.994708Z","iopub.status.idle":"2024-06-29T19:44:34.304621Z","shell.execute_reply.started":"2024-06-29T19:36:24.99468Z","shell.execute_reply":"2024-06-29T19:44:34.303301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CatBoost Classifier\nCredit to tuning method using Optuna to https://www.kaggle.com/code/zeyadsayedadbullah/predicting-academic-success-with-catboost#Build-&-Train-&-Evaluate-Model","metadata":{}},{"cell_type":"code","source":"# import optuna\n# from catboost import CatBoostClassifier, Pool\n# from sklearn.metrics import accuracy_score\n\n# # Identify categorical features for CatBoost\n# cat_features = []\n\n# # Define objective function for Optuna\n# def objective(trial):\n#     # Define hyperparameters to search\n#     params = {\n#         'boosting_type': 'Plain',\n#         'bootstrap_type':'Bernoulli',\n#         'random_strength': 0.078,\n#         'max_bin': trial.suggest_int('max_bin', 100, 500),\n#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n#         'depth': trial.suggest_int('depth', 3, 10),\n#         'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.1, 10),\n#         'eval_metric': 'Accuracy',\n#         'verbose': False,\n#         'iterations': trial.suggest_int('iterations', 100, 1000),\n#         'task_type': 'CPU'  \n#     }\n\n#     # Create training and validation pools\n#     train_pool = Pool(X_train, y_train, cat_features=cat_features)\n#     val_pool = Pool(X_valid, y_valid, cat_features=cat_features)\n\n#     # Train CatBoost model with current hyperparameters\n#     clf = CatBoostClassifier(**params)\n#     clf.fit(train_pool, eval_set=val_pool, verbose=300)\n\n#     # Calculate evaluation metric on validation set\n#     auc = clf.get_best_score()['validation']['Accuracy']\n#     return auc\n\n# # Optimize hyperparameters using Optuna\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=50)\n\n# # Get best hyperparameters\n# best_params = study.best_params\n# print(\"Best Hyperparameters:\", best_params)\n\n# # Create training and validation pools\n# train_pool = Pool(X_train, y_train, cat_features=cat_features)\n# val_pool = Pool(X_valid, y_valid, cat_features=cat_features)\n\n# # Train final model with best hyperparameters\n# catboost_normal = CatBoostClassifier(**best_params)\n# catboost_normal.fit(train_pool, eval_set=val_pool, verbose=300)\n# y_pred_catboost = catboost_normal.predict(X_valid)\n\n# normal_cat_acc = accuracy_score(y_valid, y_pred_catboost)\n\n# print(\"Test ACC:\", normal_cat_acc)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-29T15:55:52.189808Z","iopub.execute_input":"2024-06-29T15:55:52.19018Z","iopub.status.idle":"2024-06-29T15:55:52.196737Z","shell.execute_reply.started":"2024-06-29T15:55:52.190149Z","shell.execute_reply":"2024-06-29T15:55:52.195671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Best CatBoost Hyperparameters:**\n> {'max_bin': 104, 'learning_rate': 0.06781082004813864, 'depth': 9, 'l2_leaf_reg': 5.722837557334016, 'iterations': 832}","metadata":{}},{"cell_type":"code","source":"# Create CatBoost model\ncatb_1 = CatBoostClassifier(boosting_type = 'Plain',\n                       bootstrap_type = 'Bernoulli',\n                       colsample_bylevel = 0.638,\n                       learning_rate = 0.09,\n                       random_strength = 0.078,\n                       max_bin = 490,\n                       depth = 5,\n                       subsample = 0.843,\n                       l2_leaf_reg = 5,\n                       verbose = 0)\n\n# Perform cross-validation\ncv_scores = cross_val_score(catb_1, X_train, y_train, cv=5, scoring='accuracy')\n\n# Print CV scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean())  #0.8303002680712576\n\n# Fit the model on the full training data\ncatb_1.fit(X_train, y_train)\n\n# Predict on validation data\ny_pred = catb_1.predict(X_valid)\n\n# Calculate accuracy on validation data\naccuracy = accuracy_score(y_valid, y_pred)\nprint(\"Accuracy on Validation Data:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:44:34.306022Z","iopub.execute_input":"2024-06-29T19:44:34.306347Z","iopub.status.idle":"2024-06-29T19:47:00.044776Z","shell.execute_reply.started":"2024-06-29T19:44:34.30632Z","shell.execute_reply":"2024-06-29T19:47:00.0434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CatBoost model\ncatb_2 = CatBoostClassifier(boosting_type = 'Plain',\n                       bootstrap_type = 'Bernoulli',\n                       learning_rate = 0.06781082004813864,\n                       max_bin = 104,\n                       depth = 9,\n                       subsample = 0.843,\n                       l2_leaf_reg = 5.722837557334016,\n                       iterations = 832,\n                       verbose = 0)\n\n# Perform cross-validation\ncv_scores = cross_val_score(catb_2, X_train, y_train, cv=5, scoring='accuracy')\n\n# Print CV scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean())  #0.8305126583407663\n\n# Fit the model on the full training data\ncatb_2.fit(X_train, y_train)\n\n# Predict on validation data\ny_pred = catb_2.predict(X_valid)\n\n# Calculate accuracy on validation data\naccuracy = accuracy_score(y_valid, y_pred)\nprint(\"Accuracy on Validation Data:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:47:00.046348Z","iopub.execute_input":"2024-06-29T19:47:00.046818Z","iopub.status.idle":"2024-06-29T19:54:47.641563Z","shell.execute_reply.started":"2024-06-29T19:47:00.046775Z","shell.execute_reply":"2024-06-29T19:54:47.6403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM Classifier","metadata":{}},{"cell_type":"code","source":"# import optuna\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score\n\n# # Define objective function for Optuna\n# def objective(trial):\n#     # Define hyperparameters to search\n#     params = {\n#         'max_depth': trial.suggest_int('max_depth', 3, 10),\n#         'n_estimators': trial.suggest_int('n_estimators', 500, 2500),\n#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 50),\n#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n#         'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10.0),\n#         'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0),\n#         'feature_fraction': trial.suggest_float('feature_fraction', 0.01, 1),\n#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n#         'max_bin': trial.suggest_int('max_bin', 100, 500),\n#         'random_state': 0\n#     }\n\n#     # Split the training data into training and validation sets\n#     X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n#     # Train XGBoost model with current hyperparameters\n#     clf = LGBMClassifier(**params)\n#     clf.fit(X_train_split, y_train_split)\n\n#     # Predict on validation set\n#     y_pred = clf.predict(X_valid_split)\n\n#     # Calculate evaluation metric on validation set\n#     accuracy = accuracy_score(y_valid_split, y_pred)\n#     return accuracy\n\n# # Optimize hyperparameters using Optuna\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=50)\n\n# # Get perperparameters\n# best_params = study.best_params\n# print(\"Best Hyperparameters:\", best_params)\n\n# # Train final model with best hyperparameters\n# lgb = LGBMClassifier(**best_params)\n# lgb.fit(X_train, y_train)\n\n# # Predict on validation data\n# y_pred_lgb = lgb.predict(X_valid)\n\n# # Calculate accuracy on validation data\n# accuracy = accuracy_score(y_valid, y_pred_lgb)\n# print(\"Accuracy on Validation Data:\", accuracy)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-29T16:05:49.18233Z","iopub.execute_input":"2024-06-29T16:05:49.182627Z","iopub.status.idle":"2024-06-29T16:05:49.188689Z","shell.execute_reply.started":"2024-06-29T16:05:49.182601Z","shell.execute_reply":"2024-06-29T16:05:49.187549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Best LGBM Hyperparameters:** \n> {'max_depth': 8, 'n_estimators': 1298, 'learning_rate': 0.05892446698965526, 'min_data_in_leaf': 13, 'colsample_bytree': 0.918885930828695, 'reg_alpha': 3.413935089089325, 'reg_lambda': 3.9519248166416734, 'feature_fraction': 0.19287521932202015, 'subsample': 0.8246376337258945, 'max_bin': 428}\n\n","metadata":{}},{"cell_type":"code","source":"# Create LGBM model\nlgb_1 = LGBMClassifier(n_estimators = 1298,\n                    max_depth = 8,\n                    learning_rate = 0.05892446698965526,\n                    colsample_bytree = 0.918885930828695,\n                    reg_alpha = 3.413935089089325,\n                    reg_lambda = 3.9519248166416734,\n                    min_data_in_leaf = 13,\n                    subsample = 0.8246376337258945,\n                    max_bin = 428,\n                    feature_fraction = 0.19287521932202015,\n                    verbosity = -1)\n\n# Perform cross-validation\ncv_scores = cross_val_score(lgb_1, X_train, y_train, cv=5, scoring='accuracy')\n\n# Print CV scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean()) #0.8323423179658904\n\n# Fit the model on the full training data\nlgb_1.fit(X_train, y_train)\n\n# Predict on validation data\ny_pred = lgb_1.predict(X_valid)\n\n# Calculate accuracy on validation data\naccuracy = accuracy_score(y_valid, y_pred)\nprint(\"Accuracy on Validation Data:\", accuracy)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-29T19:54:47.643061Z","iopub.execute_input":"2024-06-29T19:54:47.643406Z","iopub.status.idle":"2024-06-29T19:58:51.156318Z","shell.execute_reply.started":"2024-06-29T19:54:47.643375Z","shell.execute_reply":"2024-06-29T19:58:51.154998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_params = {\n                 'objective': 'multiclass', # multiclass target: 'Graduated', 'Dropout', or 'Enrolled'\n                 'data_sample_strategy': 'goss', # Gradient-based One-Sided Sampling\n                 'tree_learner': 'feature', # split nodes based on the best feature\n                 'n_estimators': 743, # number of boosting iterations\n                 'learning_rate': 0.02636616162598401, # step size for updatig model weights\n                 'feature_fraction': 0.298183729482288, # about 30% of features considered at each split\n                 'lambda_l1': 8.242410039948067e-07, # L1 regulation penalization - adding magnitude of weights to the loss\n                 'lambda_l2': 0.4063299210212167, # L2 regulation penalization = adding the square of weights to the loss\n                 'num_leaves': 699, # Maximum number of leaves (terminal nodes) to use\n                 'max_depth': 8, # Maximum tree depth (levels) allowed\n                 'colsample_bytree': 0.7975468653525116, # proportion of samples to randomly choose at each iteration\n                 'min_child_samples': 102, # Minimum number of samples needed per leaf\n#                  'min_sum_hessian_in_leaf': 5.440582524630883, # Minimum sum of squared gradients allowed in a leaf node\n                 'min_gain_to_split': 0.7247318987185962, # Minumum gain (model score improvement) to make further leaf partitions\n                 'max_bin': 156, # Maximum numer of bins used for discretitizing features before tree splits\n                 'top_rate': 0.6132659772851583, # Top proportion of features to choose (~61%)\n                 'verbose': -1, # Turn off warnings and model logs for a cleaner look\n                 'random_state': 1 # Random state value for repeatablity\n}\n\n# Create LGBM model\nlgb_2 = LGBMClassifier(**lgbm_params)\n\n# Perform cross-validation\ncv_scores = cross_val_score(lgb_2, X_train, y_train, cv=5, scoring='accuracy')\n\n# Print CV scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean()) #0.8322769664978515\n\n# Fit the model on the full training data\nlgb_2.fit(X_train, y_train)\n\n# Predict on validation data\ny_pred = lgb_2.predict(X_valid)\n\n# Calculate accuracy on validation data\naccuracy = accuracy_score(y_valid, y_pred)\nprint(\"Accuracy on Validation Data:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:58:51.157925Z","iopub.execute_input":"2024-06-29T19:58:51.158264Z","iopub.status.idle":"2024-06-29T20:01:23.621774Z","shell.execute_reply.started":"2024-06-29T19:58:51.158234Z","shell.execute_reply":"2024-06-29T20:01:23.620536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Create LGBM model\n# params = {'n_estimators': 8000, \n#           'num_class': 3,\n#           'boosting_type': 'gbdt',\n#           'objective': 'multiclass',\n#           'metric': 'multi_logloss',\n#           'verbosity': -1,\n#           'random_state': 99, \n#           'reg_alpha': 1.7878527151970849, \n#           'reg_lambda': 1.391543710164331, \n#           'colsample_bytree': 0.5, \n#           'subsample': 0.5, \n#           'learning_rate': 0.04, \n#           'max_depth': 20, \n#           'num_leaves': 70, \n#           'min_child_samples': 40, \n#           'min_data_per_groups': 16\n#          }\n\n# lgb_3 = LGBMClassifier(**params)\n\n# # Perform cross-validation\n# cv_scores = cross_val_score(lgb_3, X_train, y_train, cv=5, scoring='accuracy')\n\n# # Print CV scores\n# print(\"Cross-Validation Scores:\", cv_scores)\n# print(\"Mean CV Score:\", cv_scores.mean()) #0.8323423179658904\n\n# # Fit the model on the full training data\n# lgb_3.fit(X_train, y_train)\n\n# # Predict on validation data\n# y_pred = lgb.predict(X_valid)\n\n# # Calculate accuracy on validation data\n# accuracy = accuracy_score(y_valid, y_pred)\n# print(\"Accuracy on Validation Data:\", accuracy)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-29T16:12:02.508118Z","iopub.execute_input":"2024-06-29T16:12:02.508432Z","iopub.status.idle":"2024-06-29T16:12:02.513533Z","shell.execute_reply.started":"2024-06-29T16:12:02.508405Z","shell.execute_reply":"2024-06-29T16:12:02.512566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create LGBM model\n\nlgb_3 = LGBMClassifier(n_estimators = 1894,\n                    max_depth = 23,\n                    learning_rate = 0.024309983270196903,\n                    min_data_in_leaf = 27,\n                    subsample = 0.40065361124232945,\n                    max_bin = 267,\n                    feature_fraction = 0.1326832138080814,\n                    verbosity = -1)\n\n# Perform cross-validation\ncv_scores = cross_val_score(lgb_3, X_train, y_train, cv=5, scoring='accuracy')\n\n# Print CV scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean()) #0.8319665637048109\n\n# Fit the model on the full training data\nlgb_3.fit(X_train, y_train)\n\n# Predict on validation data\ny_pred = lgb_3.predict(X_valid)\n\n# Calculate accuracy on validation data\naccuracy = accuracy_score(y_valid, y_pred)\nprint(\"Accuracy on Validation Data:\", accuracy)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-29T20:01:23.625145Z","iopub.execute_input":"2024-06-29T20:01:23.625494Z","iopub.status.idle":"2024-06-29T20:06:17.962224Z","shell.execute_reply.started":"2024-06-29T20:01:23.625463Z","shell.execute_reply":"2024-06-29T20:06:17.960634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Networks\nCredit - https://www.kaggle.com/code/satyaprakashshukl/simple-neural-anlysis-blend-sol","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2024-06-29T20:06:17.963878Z","iopub.execute_input":"2024-06-29T20:06:17.964232Z","iopub.status.idle":"2024-06-29T20:06:32.085699Z","shell.execute_reply.started":"2024-06-29T20:06:17.964201Z","shell.execute_reply":"2024-06-29T20:06:32.08442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to categorical (one-hot encoding)\ny_train_categorical = to_categorical(y_train)\ny_valid_categorical = to_categorical(y_valid)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T20:06:32.087769Z","iopub.execute_input":"2024-06-29T20:06:32.088552Z","iopub.status.idle":"2024-06-29T20:06:32.096784Z","shell.execute_reply.started":"2024-06-29T20:06:32.088514Z","shell.execute_reply":"2024-06-29T20:06:32.095199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the neural network model\nmodel = Sequential()\nmodel.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(y_train_categorical.shape[1], activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train, y_train_categorical, epochs=50, batch_size=32, validation_data=(X_valid, y_valid_categorical))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_valid, y_valid_categorical)\nprint(\"Validation Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T20:06:32.101283Z","iopub.execute_input":"2024-06-29T20:06:32.101728Z","iopub.status.idle":"2024-06-29T20:10:49.791061Z","shell.execute_reply.started":"2024-06-29T20:06:32.101694Z","shell.execute_reply":"2024-06-29T20:10:49.789751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict with Neural Network\nnn_pred_probs = model.predict(X_valid)\nnn_pred = np.argmax(nn_pred_probs, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T20:10:49.792533Z","iopub.execute_input":"2024-06-29T20:10:49.792926Z","iopub.status.idle":"2024-06-29T20:10:51.136591Z","shell.execute_reply.started":"2024-06-29T20:10:49.792895Z","shell.execute_reply":"2024-06-29T20:10:51.135395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importance Feature","metadata":{}},{"cell_type":"code","source":"# # Feature importance\n# feature_importances = xgb_1.feature_importances_\n# features = X.columns\n# xgb_1_feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n# xgb_1_feature_importance_df = xgb_1_feature_importance_df.sort_values(by='Importance', ascending=False)\n\n# # Display feature importance\n# print(\"\\nFeature Importances:\")\n# print(xgb_1_feature_importance_df)\n\n# # Plot feature importance\n# plt.figure(figsize=(12, 8))\n# sns.barplot(x='Importance', y='Feature', data=xgb_1_feature_importance_df)\n# plt.title('Feature Importance for XGBoost Model')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.835204Z","iopub.execute_input":"2024-06-29T05:15:43.835588Z","iopub.status.idle":"2024-06-29T05:15:43.840557Z","shell.execute_reply.started":"2024-06-29T05:15:43.835553Z","shell.execute_reply":"2024-06-29T05:15:43.839544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Feature importance\n# feature_importances = xgb_2.feature_importances_\n# features = X.columns\n# xgb_2_feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n# xgb_2_feature_importance_df = xgb_2_feature_importance_df.sort_values(by='Importance', ascending=False)\n\n# # Feature importance\n# feature_importances = xgb_3.feature_importances_\n# features = X.columns\n# xgb_3_feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n# xgb_3_feature_importance_df = xgb_3_feature_importance_df.sort_values(by='Importance', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.8418Z","iopub.execute_input":"2024-06-29T05:15:43.842098Z","iopub.status.idle":"2024-06-29T05:15:43.850759Z","shell.execute_reply.started":"2024-06-29T05:15:43.842072Z","shell.execute_reply":"2024-06-29T05:15:43.849806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# important_xgb_1_features = xgb_1_feature_importance_df[xgb_1_feature_importance_df['Importance'] > 0.008]['Feature'].tolist()\n# important_xgb_2_features = xgb_2_feature_importance_df[xgb_2_feature_importance_df['Importance'] > 0.008]['Feature'].tolist()\n# important_xgb_3_features = xgb_3_feature_importance_df[xgb_2_feature_importance_df['Importance'] > 0.008]['Feature'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.851933Z","iopub.execute_input":"2024-06-29T05:15:43.852208Z","iopub.status.idle":"2024-06-29T05:15:43.864377Z","shell.execute_reply.started":"2024-06-29T05:15:43.852185Z","shell.execute_reply":"2024-06-29T05:15:43.863298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Feature importance\n# feature_importances = lgb_1.feature_importances_\n# features = X.columns\n# lgb_1_feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n# lgb_1_feature_importance_df = lgb_1_feature_importance_df.sort_values(by='Importance', ascending=False)\n\n# # Display feature importance\n# print(\"\\nFeature Importances:\")\n# print(lgb_1_feature_importance_df)\n\n# # Plot feature importance\n# plt.figure(figsize=(12, 8))\n# sns.barplot(x='Importance', y='Feature', data=lgb_1_feature_importance_df)\n# plt.title('Feature Importance for LightGBM Model')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.865576Z","iopub.execute_input":"2024-06-29T05:15:43.865889Z","iopub.status.idle":"2024-06-29T05:15:43.873507Z","shell.execute_reply.started":"2024-06-29T05:15:43.865862Z","shell.execute_reply":"2024-06-29T05:15:43.872467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Feature importance\n# feature_importances = lgb_2.feature_importances_\n# features = X.columns\n# lgb_2_feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n# lgb_2_feature_importance_df = lgb_2_feature_importance_df.sort_values(by='Importance', ascending=False)\n\n# # Feature importance\n# feature_importances = lgb_3.feature_importances_\n# features = X.columns\n# lgb_3_feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n# lgb_3_feature_importance_df = lgb_3_feature_importance_df.sort_values(by='Importance', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.874684Z","iopub.execute_input":"2024-06-29T05:15:43.875004Z","iopub.status.idle":"2024-06-29T05:15:43.883425Z","shell.execute_reply.started":"2024-06-29T05:15:43.874973Z","shell.execute_reply":"2024-06-29T05:15:43.882409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# important_lgb_1_features = lgb_1_feature_importance_df[lgb_1_feature_importance_df['Importance'] > 1000]['Feature'].tolist()\n# important_lgb_2_features = lgb_2_feature_importance_df[lgb_2_feature_importance_df['Importance'] > 1000]['Feature'].tolist()\n# important_lgb_3_features = lgb_3_feature_importance_df[lgb_3_feature_importance_df['Importance'] > 1000]['Feature'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.884607Z","iopub.execute_input":"2024-06-29T05:15:43.88494Z","iopub.status.idle":"2024-06-29T05:15:43.895225Z","shell.execute_reply.started":"2024-06-29T05:15:43.884908Z","shell.execute_reply":"2024-06-29T05:15:43.894256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Feature importance\n# feature_importances = catb_1.feature_importances_\n# features = X.columns\n# catb_1_feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n# catb_1_feature_importance_df = catb_1_feature_importance_df.sort_values(by='Importance', ascending=False)\n\n# # Display feature importance\n# print(\"\\nFeature Importances:\")\n# print(catb_1_feature_importance_df)\n\n# # Plot feature importance\n# plt.figure(figsize=(12, 8))\n# sns.barplot(x='Importance', y='Feature', data=catb_1_feature_importance_df)\n# plt.title('Feature Importance for CatBoost Model')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.896302Z","iopub.execute_input":"2024-06-29T05:15:43.89658Z","iopub.status.idle":"2024-06-29T05:15:43.904896Z","shell.execute_reply.started":"2024-06-29T05:15:43.896551Z","shell.execute_reply":"2024-06-29T05:15:43.903662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Feature importance\n# feature_importances = catb_2.feature_importances_\n# features = X.columns\n# catb_2_feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n# catb_2_feature_importance_df = catb_2_feature_importance_df.sort_values(by='Importance', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.906039Z","iopub.execute_input":"2024-06-29T05:15:43.906368Z","iopub.status.idle":"2024-06-29T05:15:43.913925Z","shell.execute_reply.started":"2024-06-29T05:15:43.906342Z","shell.execute_reply":"2024-06-29T05:15:43.912662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# important_catb_1_features = catb_1_feature_importance_df[catb_1_feature_importance_df['Importance'] > 2]['Feature'].tolist()\n# important_catb_2_features = catb_2_feature_importance_df[catb_2_feature_importance_df['Importance'] > 2]['Feature'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.915234Z","iopub.execute_input":"2024-06-29T05:15:43.915536Z","iopub.status.idle":"2024-06-29T05:15:43.923612Z","shell.execute_reply.started":"2024-06-29T05:15:43.91551Z","shell.execute_reply":"2024-06-29T05:15:43.922676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# important_features = []\n# def select_features(feature_list):\n#     for f in feature_list:\n#         if f not in important_features:\n#             important_features.append(f)\n#     return important_features\n\n# select_features(important_xgb_2_features)\n# select_features(important_xgb_3_features)\n# select_features(important_catb_1_features)\n# important_features = list(set(important_features))","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.930652Z","iopub.execute_input":"2024-06-29T05:15:43.931021Z","iopub.status.idle":"2024-06-29T05:15:43.936462Z","shell.execute_reply.started":"2024-06-29T05:15:43.930992Z","shell.execute_reply":"2024-06-29T05:15:43.935371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# important_features.sort()\n# important_features","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.937466Z","iopub.execute_input":"2024-06-29T05:15:43.937782Z","iopub.status.idle":"2024-06-29T05:15:43.945612Z","shell.execute_reply.started":"2024-06-29T05:15:43.937734Z","shell.execute_reply":"2024-06-29T05:15:43.944648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"code","source":"# X_train_important = X_train[important_features]\n# X_valid_important = X_valid[important_features]","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.947119Z","iopub.execute_input":"2024-06-29T05:15:43.947434Z","iopub.status.idle":"2024-06-29T05:15:43.955366Z","shell.execute_reply.started":"2024-06-29T05:15:43.947408Z","shell.execute_reply":"2024-06-29T05:15:43.954251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgb_1.fit(X_train_important, y_train)\n# xgb_2.fit(X_train_important, y_train)\n# xgb_3.fit(X_train_important, y_train)\n# # Predict on validation data\n# y_pred_1 = xgb_1.predict(X_valid_important)\n# y_pred_2 = xgb_2.predict(X_valid_important)\n# y_pred_3 = xgb_3.predict(X_valid_important)\n\n# # Calculate accuracy on validation data\n# accuracy = accuracy_score(y_valid, y_pred_1)\n# print(\"Accuracy on Validation Data:\", accuracy)\n\n# accuracy = accuracy_score(y_valid, y_pred_2)\n# print(\"Accuracy on Validation Data:\", accuracy)\n\n# accuracy = accuracy_score(y_valid, y_pred_3)\n# print(\"Accuracy on Validation Data:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.956758Z","iopub.execute_input":"2024-06-29T05:15:43.957084Z","iopub.status.idle":"2024-06-29T05:15:43.964319Z","shell.execute_reply.started":"2024-06-29T05:15:43.957058Z","shell.execute_reply":"2024-06-29T05:15:43.963327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lgb_1.fit(X_train_important, y_train)\n# lgb_2.fit(X_train_important, y_train)\n# lgb_3.fit(X_train_important, y_train)\n# # Predict on validation data\n# y_pred_1 = lgb_1.predict(X_valid_important)\n# y_pred_2 = lgb_2.predict(X_valid_important)\n# y_pred_3 = lgb_3.predict(X_valid_important)\n\n# # Calculate accuracy on validation data\n# accuracy = accuracy_score(y_valid, y_pred_1)\n# print(\"Accuracy on Validation Data:\", accuracy)\n\n# accuracy = accuracy_score(y_valid, y_pred_2)\n# print(\"Accuracy on Validation Data:\", accuracy)\n\n# accuracy = accuracy_score(y_valid, y_pred_3)\n# print(\"Accuracy on Validation Data:\", accuracy)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-29T05:15:43.965697Z","iopub.execute_input":"2024-06-29T05:15:43.96614Z","iopub.status.idle":"2024-06-29T05:15:43.975193Z","shell.execute_reply.started":"2024-06-29T05:15:43.966113Z","shell.execute_reply":"2024-06-29T05:15:43.974029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# catb_1.fit(X_train_important, y_train)\n# catb_2.fit(X_train_important, y_train)\n\n# # Predict on validation data\n# y_pred_1 = catb_1.predict(X_valid_important)\n# y_pred_2 = catb_2.predict(X_valid_important)\n\n# # Calculate accuracy on validation data\n# accuracy = accuracy_score(y_valid, y_pred_1)\n# print(\"Accuracy on Validation Data:\", accuracy)\n\n# accuracy = accuracy_score(y_valid, y_pred_2)\n# print(\"Accuracy on Validation Data:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:15:43.976388Z","iopub.execute_input":"2024-06-29T05:15:43.976669Z","iopub.status.idle":"2024-06-29T05:15:43.988098Z","shell.execute_reply.started":"2024-06-29T05:15:43.976645Z","shell.execute_reply":"2024-06-29T05:15:43.987063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Models\nI learned about ensembling methods from this notebook (credit) - https://www.kaggle.com/code/waleedejaz/eda-models-ensemble#11.-Ensemble-Models\n\nThree techniques are used for Ensemble:\n\n* Majority (Hard) Voting\n* Soft Voting\n* Weighted Average\n\nWe will use all of them on our dataset and through evaluation, we will pick the Ensemble method that have the highest validation score.","metadata":{}},{"cell_type":"markdown","source":"## Voting\n### Majority Voting","metadata":{}},{"cell_type":"code","source":"xgb_pred_1 = xgb_1.predict(X_valid)\nxgb_pred_2 = xgb_2.predict(X_valid)\nxgb_pred_3 = xgb_3.predict(X_valid)\ncatboost_pred_1 = catb_1.predict_proba(X_valid).argmax(axis=1)\ncatboost_pred_2 = catb_2.predict_proba(X_valid).argmax(axis=1)\nlgbm_pred_1 = lgb_1.predict(X_valid)\nlgbm_pred_2 = lgb_2.predict(X_valid)\nlgbm_pred_3 = lgb_3.predict(X_valid)\nrf_pred = rf.predict(X_valid)\nknn_pred = knn.predict(X_valid)\n\nensemble_test_predictions = pd.DataFrame({\n    'XGBoost_1': xgb_pred_1,\n    'XGBoost_2': xgb_pred_2,\n    'XGBoost_3': xgb_pred_3,\n    'LightGBM_1': lgbm_pred_1,\n    'LightGBM_2': lgbm_pred_2,\n    'LightGBM_3': lgbm_pred_3,\n    'CatBoost_1': catboost_pred_1,\n    'CatBoost_2': catboost_pred_2,\n    'RF': rf_pred,\n    'KNN': knn_pred,\n    'NeuralNet': nn_pred\n})\n\nmajority_voting_test_predictions = ensemble_test_predictions.mode(axis=1).iloc[:, 0] #0.8364479874542603\n\nprint(f\"Majority(Hard) Voting Accuracy: {accuracy_score(y_valid, majority_voting_test_predictions)}\") ","metadata":{"execution":{"iopub.status.busy":"2024-06-29T20:10:51.138197Z","iopub.execute_input":"2024-06-29T20:10:51.138666Z","iopub.status.idle":"2024-06-29T20:12:36.286001Z","shell.execute_reply.started":"2024-06-29T20:10:51.138625Z","shell.execute_reply":"2024-06-29T20:12:36.284799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Soft Voting","metadata":{}},{"cell_type":"code","source":"# Predict probabilities for each model\nxgb_1_probs = xgb_1.predict_proba(X_valid)\nxgb_2_probs = xgb_2.predict_proba(X_valid)\nxgb_3_probs = xgb_3.predict_proba(X_valid)\nlgbm_1_probs = lgb_1.predict_proba(X_valid)\nlgbm_2_probs = lgb_2.predict_proba(X_valid)\nlgbm_3_probs = lgb_3.predict_proba(X_valid)\ncatboost_1_probs = catb_1.predict_proba(X_valid)\ncatboost_2_probs = catb_2.predict_proba(X_valid)\nrf_probs = rf.predict_proba(X_valid)\nknn_probs = knn.predict_proba(X_valid)\n\n# Soft Voting\nsoft_voting_probs = (catboost_1_probs + catboost_2_probs +\n                     xgb_1_probs + xgb_2_probs + xgb_3_probs +\n                     lgbm_1_probs + lgbm_2_probs + lgbm_3_probs +\n                     rf_probs +\n                     knn_probs +\n                     nn_pred_probs) / 10\nsoft_voting_predictions = soft_voting_probs.argmax(axis=1)\n\nprint(f\"Soft Voting Accuracy: {accuracy_score(y_valid, soft_voting_predictions)}\") ","metadata":{"execution":{"iopub.status.busy":"2024-06-29T20:12:36.288595Z","iopub.execute_input":"2024-06-29T20:12:36.288947Z","iopub.status.idle":"2024-06-29T20:14:15.820519Z","shell.execute_reply.started":"2024-06-29T20:12:36.288916Z","shell.execute_reply":"2024-06-29T20:14:15.819314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Weighted Average","metadata":{}},{"cell_type":"code","source":"weighted_avg_probs = (0.3 * (catboost_1_probs + catboost_2_probs) / 2 +\n                      0.1 * (xgb_1_probs + xgb_2_probs + xgb_3_probs) / 3 +\n                      0.4 * (lgbm_1_probs + lgbm_2_probs + lgbm_3_probs) / 3 +\n                      0.05 * rf_probs +\n                      0.05 * knn_probs +\n                      0.1 * nn_pred_probs)\nweighted_avg_predictions = weighted_avg_probs.argmax(axis=1)\n\nprint(f\"Weighted Average Accuracy: {accuracy_score(y_valid, weighted_avg_predictions)}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T20:14:15.821806Z","iopub.execute_input":"2024-06-29T20:14:15.822129Z","iopub.status.idle":"2024-06-29T20:14:15.834121Z","shell.execute_reply.started":"2024-06-29T20:14:15.8221Z","shell.execute_reply":"2024-06-29T20:14:15.833022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking\nStacking did not yield high accuracy score while taking long computing time, so I will comment it out.","metadata":{}},{"cell_type":"code","source":"# # Define the base models\n# c1 = xgb\n# c2 = lgb\n# c3 = catb\n# c4 = SVC(probability=True)\n# c5 = RandomForestClassifier()\n# c6 = KNeighborsClassifier()\n\n# # Define the stacking ensemble with base estimators and final estimator\n# estimators = [\n#     ('xgb', c1),\n#     ('lgb', c2),\n#     ('catb', c3),\n#     ('svm', c4),\n#     ('rf', c5),\n#     ('knn', c6)\n# ]\n\n# from sklearn.ensemble import StackingClassifier\n# stacking_model = StackingClassifier(estimators=estimators, final_estimator=lgb, cv = KFold(n_splits = 5))\n\n# # Fit the stacking model\n# stacking_model.fit(X_train, y_train) \n\n# # Predict with the stacking model\n# y_pred_stacking = stacking_model.predict(X_valid)  \n\n# # Evaluate the accuracy of the stacking model\n# accuracy_stacking = accuracy_score(y_valid, y_pred_stacking)\n\n# print(f\"Stacking Accuracy:, {accuracy_score(y_valid, y_pred_stacking)}\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-29T05:18:59.396825Z","iopub.execute_input":"2024-06-29T05:18:59.397207Z","iopub.status.idle":"2024-06-29T05:18:59.402257Z","shell.execute_reply.started":"2024-06-29T05:18:59.397178Z","shell.execute_reply":"2024-06-29T05:18:59.401314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.ensemble import StackingClassifier\n# from sklearn.metrics import roc_auc_score\n# base_models=[\n#     ('xgb',xgb_1),\n#     ('lgbm',lgb_2),\n#     ('catb', catb_1),\n#     ('knn', knn),\n#     ('rf', rf)\n# ]\n# stacking_clf = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n\n# stacking_clf.fit(X_train, y_train)\n\n# # Predict with the stacking model\n# y_pred_stacking = stacking_clf.predict(X_valid)  \n\n# # Evaluate the accuracy of the stacking model\n# accuracy_stacking = accuracy_score(y_valid, y_pred_stacking)\n# roc_auc = roc_auc_score(y_valid, stacking_clf.predict_proba(X_valid), multi_class='ovr')\n# print(f\"Stacking Accuracy:, {accuracy_stacking}\")\n# print(f'ROC AUC Score: {roc_auc:.2f}')","metadata":{"execution":{"iopub.status.busy":"2024-06-29T16:16:41.332321Z","iopub.execute_input":"2024-06-29T16:16:41.332624Z","iopub.status.idle":"2024-06-29T17:00:00.164184Z","shell.execute_reply.started":"2024-06-29T16:16:41.332598Z","shell.execute_reply":"2024-06-29T17:00:00.16108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate","metadata":{}},{"cell_type":"code","source":"ensemble_accuracy = {\n    'Majority Voting': accuracy_score(y_valid,  majority_voting_test_predictions),\n    'Soft Voting': accuracy_score(y_valid, soft_voting_predictions),\n    'Weighted Average': accuracy_score(y_valid, weighted_avg_predictions)\n}\n\nprint(\"Ensemble Accuracy:\")\nfor method, acc in ensemble_accuracy.items():\n    print(f\"{method}: {acc:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T20:14:15.835423Z","iopub.execute_input":"2024-06-29T20:14:15.835751Z","iopub.status.idle":"2024-06-29T20:14:15.853598Z","shell.execute_reply.started":"2024-06-29T20:14:15.835723Z","shell.execute_reply":"2024-06-29T20:14:15.852258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Confusion matrix\n# conf_matrix = confusion_matrix(y_valid, y_pred_stacking)\n# plt.figure(figsize=(8, 6))\n# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n# plt.title('Confusion Matrix')\n# plt.xlabel('Predicted')\n# plt.ylabel('Actual')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:32:47.878551Z","iopub.execute_input":"2024-06-29T05:32:47.879838Z","iopub.status.idle":"2024-06-29T05:32:48.195629Z","shell.execute_reply.started":"2024-06-29T05:32:47.879797Z","shell.execute_reply":"2024-06-29T05:32:48.194572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Predict probabilities for each model\nxgb_pred_1 = xgb_1.predict(df_test)\nxgb_pred_2 = xgb_2.predict(df_test)\nxgb_pred_3 = xgb_3.predict(df_test)\ncatboost_pred_1 = catb_1.predict_proba(df_test).argmax(axis=1)\ncatboost_pred_2 = catb_2.predict_proba(df_test).argmax(axis=1)\nlgbm_pred_1 = lgb_1.predict(df_test)\nlgbm_pred_2 = lgb_2.predict(df_test)\nlgbm_pred_3 = lgb_3.predict(df_test)\nrf_pred = rf.predict(df_test)\nknn_pred = knn.predict(df_test)\nnn_pred_probs = model.predict(df_test)\nnn_pred = np.argmax(nn_pred_probs, axis=1)\n\nensemble_test_predictions = pd.DataFrame({\n    'XGBoost_1': xgb_pred_1,\n    'XGBoost_2': xgb_pred_2,\n    'XGBoost_3': xgb_pred_3,\n    'LightGBM_1': lgbm_pred_1,\n    'LightGBM_2': lgbm_pred_2,\n    'LightGBM_3': lgbm_pred_3,\n    'CatBoost_1': catboost_pred_1,\n    'CatBoost_2': catboost_pred_2,\n    'RF': rf_pred,\n    'KNN': knn_pred,\n    'NN': nn_pred\n})\n\nmajority_voting_test_predictions = ensemble_test_predictions.mode(axis=1).iloc[:, 0] \ny_predictions = list(map(int, majority_voting_test_predictions))\n\nsample_submission_df = pd.read_csv('/kaggle/input/playground-series-s4e6/sample_submission.csv')\nsample_submission_df['Target'] = y_predictions\nsample_submission_df['Target'] = le.inverse_transform(sample_submission_df['Target'])\nsample_submission_df.to_csv('/kaggle/working/submission.csv', index=False)\nsample_submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T20:14:15.85499Z","iopub.execute_input":"2024-06-29T20:14:15.855412Z","iopub.status.idle":"2024-06-29T20:20:03.835484Z","shell.execute_reply.started":"2024-06-29T20:14:15.855377Z","shell.execute_reply":"2024-06-29T20:20:03.834302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}